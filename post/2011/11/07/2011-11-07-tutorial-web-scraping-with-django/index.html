

  <!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>Tutorial: Web scraping with Django - Kevin Schaul</title>
    <meta property="og:title" content="Tutorial: Web scraping with Django - Kevin Schaul">
    
    <meta name="twitter:card" content="summary">

    
      
    

    
      
      <meta property="description" content="For a recent MinnPost project, we wanted to scrape court dockets, so I figured I’d break out a python script in the wonderful ScraperWiki. One of my favorite features is that you can schedule a &amp;hellip;">
      <meta property="og:description" content="For a recent MinnPost project, we wanted to scrape court dockets, so I figured I’d break out a python script in the wonderful ScraperWiki. One of my favorite features is that you can schedule a &amp;hellip;">
      
    

    
    

    

    
    


    <style> @import url('https://fonts.googleapis.com/css2?family=Raleway:wght@300;500;800&display=swap'); </style>
    
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <link rel="stylesheet" href="/css/custom.css" />

  </head>

  
  <body class="post">
    <header class="masthead">
      <h1><a href="/">Kevin Schaul</a></h1>



      <nav class="menu">
  <ul>
  
  
  <li><a href="/contact/">Contact</a></li>
  
  <li><a href="/link/">Blogmarks</a></li>
  
  <li><a href="/til/">Today I Learned</a></li>
  
  <li><a href="/post/">Archive</a></li>
  
  <li><a href="/index.xml">RSS</a></li>
<li style="display: none;"><a rel="me" href="https://mastodon.garden/@kevinschaul">Mastodon</a></li>
  </ul>
</nav>

    </header>

    <article class="main">
      <header class="title">
      
<h1>Tutorial: Web scraping with Django</h1>
<h3>November 7, 2011</h3>

      </header>

  
  
  
  
  


<p>For a recent <a href="http://www.minnpost.com">MinnPost</a> project, we wanted to scrape court dockets, so I figured I’d break out a python script in the wonderful <a href="https://scraperwiki.com/">ScraperWiki</a>. One of my favorite features is that you can schedule a scraper to run automatically. One of my least favorite features is that the limit on automatic scrapers is once per day. We needed something to run every half hour.</p>
<p><strong>Enter Django</strong></p>
<p>It seems that every news hacker is using <a href="https://www.djangoproject.com/">Django</a> for something these days, and why not? It’s fast, flexible and a major headache to deploy (I’ll expand on this in a later post).</p>
<p>To build the scraper, I wrote a python script that used <a href="http://docs.python-requests.org/en/latest/index.html">requests</a> and <a href="http://lxml.de/">lxml</a>, invoked by a cron call to a Django command.</p>
<p>Here’s the site we want to scrape (and a great example of how “open” government really isn’t): <a href="http://www.mnd.uscourts.gov/calendars/mpls/index.html">Minneapolis court dockets</a></p>
<p><strong>models.py</strong></p>
<p>The models.py file is very simple, containing only the fields we want to scrape.</p>
<pre><code>from django.db import models
  
class Case(models.Model):
    start = models.DateTimeField()
    end = models.DateTimeField()
    court = models.CharField(max_length=60)
    description = models.CharField(max_length=1024)
    def __unicode__(self):
        return self.description</code></pre>
<p>This should be self-explanatory if you’re at all familiar with Django; if not, I highly recommend the <a href="https://docs.djangoproject.com/en/1.3/intro/tutorial01/">official tutorial</a>.</p>
<p><strong>The scraping script</strong></p>
<p>Using requests and lxml, scraping in python is downright enjoyable. Look how easy it is to grab a site’s source and convert it into a useful object:</p>
<pre><code>r = requests.get(url)
root = lxml.html.fromstring(r.content)</code></pre>
<p>Boom.</p>
<p>Take a look at the source code of the <a href="http://www.mnd.uscourts.gov/calendars/mpls/index.html">court dockets site</a>, and you’ll see how fun it is to scrape most government sites. The information we want to get is nested in four tables (!), all without ids or classes.</p>
<p>Luckily, one of these tables has an attribute that we can immediately jump to. Here’s the code I’m using to get the contents we want:</p>
<pre><code>import requests
import lxml
from lxml import html

# To grab the URL and convert into an lxml object ...
r = requests.get(&#39;http://www.mnd.uscourts.gov/calendars/mpls/index.html&#39;)
root = lxml.html.fromstring(r.content)
for tr in root.cssselect(&quot;table[cellpadding=1] tr&quot;)[1:]:
    tds = tr.cssselect(&quot;td&quot;)
    start = tds[1].text_content().strip()
    end = tds[2].text_content().strip()
    description = tds[3].text_content().strip()</code></pre>
<p>The text_content() function takes what’s inside an html element sans html tags, and strip() removes whitespace.</p>
<p><strong>The magic - Django commands</strong></p>
<p>Django commands are scripts that can do whatever you like, easily invoked through the command line:</p>
<pre><code>python manage.py nameofacommand</code></pre>
<p>This is great to keep everything inside a Django project, and the scripts are easily accessible. These files are stored inside your app -&gt; management -&gt; commands (my full path is minnpost/dockets/management/commands/scrapedockets.py). If you don’t have these folders already, create them, but don’t forget to add <strong>init.py</strong> files. I turned my scraping code into a command called scrapedockets.py - full code below.</p>
<pre><code>from django.core.management.base import BaseCommand
from minnpost.dockets.models import Case

import requests
import lxml
from lxml import html
import time, datetime

class Command(BaseCommand):
    help = &#39;Scrapes the sites for new dockets&#39;

    def handle(self, *args, **options):
        self.stdout.write(&#39;\nScraping started at %s\n&#39; % str(datetime.datetime.now()))

        courts = {&#39;Minneapolis&#39;: &#39;http://www.mnd.uscourts.gov/calendars/mpls/index.html&#39;, &#39;St. Paul&#39;: &#39;http://www.mnd.uscourts.gov/calendars/stp/index.html&#39;, &#39;Duluth&#39;: &#39;http://www.mnd.uscourts.gov/calendars/dul/index.html&#39;, &#39;Fergus Falls &amp; Bemidji&#39;: &#39;http://www.mnd.uscourts.gov/calendars/ff/index.html&#39;}

        for court, url in courts.iteritems():
            self.stdout.write(&#39;Scraping url: %s\n&#39; % url)
            r = requests.get(url)
            root = lxml.html.fromstring(r.content)
            # Find the correct table element, skip the first row
            for tr in root.cssselect(&#39;table[cellpadding=1] tr&#39;)[1:]:
                tds = tr.cssselect(&#39;td&#39;)
                start = tds[1].text_content().strip()
                end = tds[2].text_content().strip()
                description = tds[3].text_content().strip()
                convertedStart = convertTime(start)
                convertedEnd = convertTime(end)
                dbStart = datetime.datetime.fromtimestamp(convertedStart)
                dbEnd = datetime.datetime.fromtimestamp(convertedEnd)

                if not Case.objects.filter(start=dbStart, end=dbEnd, court=court, description=description):
                    c = Case(start=dbStart, end=dbEnd, court=court[:60], description=description[:1024])
                    c.save()

now = time.gmtime(time.time())

def convertTime(t):
    &quot;&quot;&quot;Converts times in format HH:MMPM into seconds from epoch (but in CST)&quot;&quot;&quot;
    convertedTime = time.strptime(t + &#39; &#39; + str(now.tm_mon) + &#39; &#39; + str(now.tm_mday) + &#39; &#39; + str(now.tm_year), &quot;%I:%M%p %m %d %Y&quot;)
    return time.mktime(convertedTime)
    # This used to add 5 * 60 * 60 to compensate for CST</code></pre>
<p>Django commands require a class Command that extends BaseCommand and has a function handle(). This is called when the command is invoked.</p>
<p>I wrote an (admittedly) bad function to convert the times into seconds to store them in the database. I believe I went against a general rule, which is to store times in GMT, but I don’t competely understand how Django uses the timezone settings. Help?</p>
<p>Anyway, I end up with variables for each piece of information I want to store. I check if a Case already exists with the same information, and if it doesn’t, I create it and save it to the database. I used python’s slice operator to make sure the court and description aren’t too long (according to the database setup I created in models.py).</p>
<p><strong>The magic, pt. 2 - Cron</strong></p>
<p>To make this worthwhile, we need it to run on its own every half hour. Unix systems make this simple, with a daemon called Cron. If you’re using Ubuntu, <a href="https://help.ubuntu.com/community/CronHowto">here’s a nice guide</a> (other distros will be very similar). Cron schedules scripts to be run at different intervals, and its uses are virtually limitless.</p>
<p>I created a script, scrapedockets.sh, which simply calls the Django command we just walked through.</p>
<pre><code>#!/bin/bash
python manage.py scrapedockets</code></pre>
<p>Don’t forget to make it executable:</p>
<pre><code>sudo chmod +x scrapedockets.sh</code></pre>
<p>I used a crontab on the default user to call the scrapedockets.sh Django command every half hour. Edit your crontab using the command:</p>
<pre><code>crontab -e</code></pre>
<p>Each line is something you want cron to do. Here’s what mine looks like:</p>
<pre><code>*/30 * * * * /opt/django-projects/minnpost/scripts/scrapedockets.sh &gt;&gt; /opt/log/scrapedockets.log</code></pre>
<p>Cron will run the script scrapedockets.sh every 30 minutes (any minute value evenly divisible by 30) and log output to scrapedockets.log. I encourage you to <a href="https://help.ubuntu.com/community/CronHowto">look at a guide</a> to see what the structure is.</p>
<p>If everything is set up, your Django database should start filling up with information. Build some views, and show the world what you’ve found.</p>
<p><strong>If you know a better way, please share!</strong></p>
<p>I’m far from an expert, so if you see something fishy here, leave a comment or tweet at <span class="citation">@kevinschaul</span>.</p>

  
  
    <footer>
    

    
  
  

  </footer>
  </article>
  </body>
</html>

